{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from os.path import exists\n",
    "from quality_match.types import TaskInput, TaskOutput, Answer\n",
    "\n",
    "\n",
    "REFERENCES_DATA_PATH = '../data/references.json'\n",
    "(ANONYMIZED_PROJECT_DATA_PATH) = '../data/anonymized_project.json'    \n",
    "\n",
    "def get_answer_disagree_score(task_output: TaskOutput) -> int:  \n",
    "    ans_map = {Answer.NO: -1, Answer.YES: 1, Answer.EMPTY: 0}\n",
    "    return ans_map[task_output.answer]\n",
    "\n",
    "def get_reference_key(task_input: TaskInput) -> str:\n",
    "    return task_input.image_url.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "def load_normalized_data():\n",
    "    if not exists(REFERENCES_DATA_PATH) or not exists(ANONYMIZED_PROJECT_DATA_PATH):\n",
    "        raise Exception(\"Data files not found ...\")\n",
    "    \n",
    "    references = pd.read_json(REFERENCES_DATA_PATH).T\n",
    "    anonymized_project_json_file = json.load(open(ANONYMIZED_PROJECT_DATA_PATH))\n",
    "    anonymized_project_data = anonymized_project_json_file['results']['root_node']['results']\n",
    "\n",
    "    records = []\n",
    "    for task_id, tasks in anonymized_project_data.items():\n",
    "        for record in tasks['results']:\n",
    "            record['task_id'] = task_id\n",
    "            record['answer_disagree_score'] = get_answer_disagree_score(TaskOutput.from_dict(record['task_output']))\n",
    "            ref_key = get_reference_key(TaskInput.from_dict(record['task_input']))        \n",
    "            record['reference'] = references.loc[ref_key, 'is_bicycle']\n",
    "            records.append(record)        \n",
    "       \n",
    "    data = pd.json_normalize(records)\n",
    "    \n",
    "    data.loc[data['task_output.duration_ms'] < 0, \"task_output.duration_ms\"] = 0 \n",
    "    data.astype({'task_output.duration_ms': 'int64'}) \n",
    "\n",
    "    return data, references\n",
    "\n",
    "data, references = load_normalized_data()\n",
    "data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "a. How many annotators did contribute to the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['user.id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. What are the average, min and max annotation times (durations) ? \n",
    "Feel free to add visual representations here such as graphs if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['task_output.duration_ms'].describe().apply(\"{0:.2f}\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'Value Count': 90870, 'Mean': 1289.836184, 'Std': 1124.011302, 'Min': 0.0, '25%': 887.0, '50%': 1058.0, '75%': 1328.0, 'Max': 42398.0}\n",
    "s = pd.Series(d)\n",
    "\n",
    "duration_plot = s.to_frame('data').boxplot(vert=False, figsize=(10,6),meanline=True, showmeans=True)\n",
    "plt.title(\"Summary\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Did all annotators produce the same amount of results, or are there differences? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(\"user.id\").agg(count=('user.id', 'count')).sort_values('count', ascending=False).reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Are there questions for which annotators highly disagree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = data.groupby(\"task_input.image_url\").agg(sum=('answer_disagree_score', 'sum')).reset_index()\n",
    "d1[d1['sum'] == 0]['task_input.image_url'].reset_index()\n",
    "# data[data['task_input.image_url'].isin(d2)].reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a.1. How often does 'corrupt_data'  occur in the project and do you see a trend within the annotators that made use of these options?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = data[data['task_output.corrupt_data'] == True]\n",
    "c1.groupby(\"user.id\").agg(count=('user.id', 'count')).sort_values('count', ascending=False).reset_index() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a.2. How often does 'cant_solve'  occur in the project and do you see a trend within the annotators that made use of these options?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = data[data['task_output.cant_solve'] == True]\n",
    "s1.groupby(\"user.id\").agg(count=('user.id', 'count')).sort_values('count', ascending=False).reset_index()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the reference set balanced? Please demonstrate via numbers and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts =references['is_bicycle'].value_counts()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(counts.index.astype(str), counts.values)\n",
    "plt.title('Distribution of is_bicycle Column')\n",
    "plt.xlabel('is_bicycle')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the reference set, can you identify good and bad annotators? Please use statistics and visualizations. Feel free to get creative."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to identify the good/bad annotator we can consider the following approach:\n",
    "1. Calculate the accuracy of each annotator for both good and bad datasets.\n",
    "2. Determine a threshold for what constitutes a good annotator versus a bad annotator.\n",
    "3. Use the threshold to classify each annotator as either good or bad.\n",
    "4. Validate the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(rec):\n",
    "    if rec['task_output.answer']:\n",
    "        if rec['task_output.answer'] == 'yes' and rec['reference'] is True:\n",
    "            return True\n",
    "        if rec['task_output.answer'] == 'no' and rec['reference'] is False:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "bad_annotations = data[data.apply(lambda rec: not check_answer(rec) , axis=1)].groupby('user.id').agg(count=('user.id', 'count')).sort_values('count', ascending=False).reset_index()\n",
    "good_annotations = data[data.apply(lambda rec: check_answer(rec) , axis=1)].groupby('user.id').agg(count=('user.id', 'count')).sort_values('count', ascending=False).reset_index()\n",
    "\n",
    "users = data.groupby('user.id').agg(count=('user.id', 'count')).sort_values('count', ascending=False).reset_index()\n",
    "\n",
    "users['bad_annotation_count'] = bad_annotations['count']\n",
    "users['good_annotation_count'] = good_annotations['count']\n",
    "users['accuracy_rate'] = (users['good_annotation_count'] / users['count'])\n",
    "users['error_rate'] = (users['bad_annotation_count']/ users['count']) \n",
    "sorted_accuracy = users.sort_values(\"accuracy_rate\", ascending=False)\n",
    "sorted_accuracy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the threshold depends on the specific requirements of the project and the nature of the data.\n",
    "set the threshold at 0.9 in order to ensure that  annotators are at least 90% accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "users[\"annotation_quality\"]= \"unknown\"\n",
    "users.loc[users['good_annotation_count'] / users['count'] >= threshold, 'annotation_quality'] = 'good'\n",
    "users.loc[users['bad_annotation_count'] / users['count'] >= 1 - threshold, 'annotation_quality'] = 'bad'\n",
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {'good': 'green', 'bad': 'red'}\n",
    "\n",
    "plt.scatter(users['good_annotation_count'] / users['count'], users.index, c=users['annotation_quality'].apply(lambda x: colors[x]))\n",
    "plt.title('Annotation Quality')\n",
    "plt.xlabel('Proportion of Correct Answers')\n",
    "plt.ylabel('Annotator')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
